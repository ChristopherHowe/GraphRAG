{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8d6dc7",
   "metadata": {},
   "source": [
    "# Exploratory Notebook\n",
    "Contains a whole bunch of code where I tested various things out, left as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf62c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/HybridRAG/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from db_funcs import get_all_content, get_db_con\n",
    "import dotenv\n",
    "from utils import Sentence_Extractor\n",
    "from neo4j_funcs import Neo4j_Driver\n",
    "from utils import jupyter_print_paragraph\n",
    "from EntityRelationshipExtractor import ER_Extractor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "neo4j = Neo4j_Driver()\n",
    "sentence_extractor = Sentence_Extractor()\n",
    "er_extractor=ER_Extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdc30f",
   "metadata": {},
   "source": [
    "# Dataset Visualization\n",
    "Provide some statistics on content sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a32b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 802.61\n",
      "Maximum length: 4063\n",
      "Minimum length: 169\n",
      "Standard deviation: 359.58\n",
      "\n",
      "Sample of lengths: [522, 1034, 552, 539, 856, 803, 981, 626, 696, 665, 280, 1207, 673, 797, 895, 977, 550, 610, 872, 1459]\n"
     ]
    }
   ],
   "source": [
    "with get_db_con().cursor() as cur:\n",
    "    content_ids, all_content = get_all_content(cur)\n",
    "content_lengths = [len(content) for content in all_content]\n",
    "\n",
    "print(f\"Average length: {np.mean(content_lengths):.2f}\")\n",
    "print(f\"Maximum length: {max(content_lengths)}\")\n",
    "print(f\"Minimum length: {min(content_lengths)}\")\n",
    "print(f\"Standard deviation: {np.std(content_lengths):.2f}\")\n",
    "print(f\"\\nSample of lengths: {random.sample(content_lengths, min(20, len(content_lengths)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02588a57",
   "metadata": {},
   "source": [
    "# Example Entity Relationship Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b072679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[triplet(head='Bob', type='spouse', tail='Sue'), triplet(head='Sue', type='spouse', tail='Bob')]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"bob is married to sue\"\n",
    "triplets = er_extractor.extract_ERs([sentence])\n",
    "print(triplets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0fbea",
   "metadata": {},
   "source": [
    "# Sentence vs Paragraph Entity Relationship Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a815b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Paragraph:\n",
      "\n",
      "Warsaw, especially its city centre (Śródmieście), is home not only to many natio\n",
      "nal institutions and government agencies, but also to many domestic and internat\n",
      "ional companies. In 2006, 304,016 companies were registered in the city. Warsaw'\n",
      "s ever-growing business community has been noticed globally, regionally, and nat\n",
      "ionally. MasterCard Emerging Market Index has noted Warsaw's economic strength a\n",
      "nd commercial center. Moreover, Warsaw was ranked as the 7th greatest emerging m\n",
      "arket. Foreign investors' financial participation in the city's development was \n",
      "estimated in 2002 at over 650 million euro. Warsaw produces 12% of Poland's nati\n",
      "onal income, which in 2008 was 305.1% of the Polish average, per capita (or 160%\n",
      " of the European Union average). The GDP per capita in Warsaw amounted to PLN 94\n",
      " 000 in 2008 (c. EUR 23 800, USD 33 000). Total nominal GDP of the city in 2010 \n",
      "amounted to 191.766 billion PLN, 111696 PLN per capita, which was 301,1 % of Pol\n",
      "ish average. Warsaw leads the region of East-Central Europe in foreign investmen\n",
      "t and in 2006, GDP growth met expectations with a level of 6.1%. It also has one\n",
      " of the fastest growing economies, with GDP growth at 6.5 percent in 2007 and 6.\n",
      "1 percent in the first quarter of 2008.\n",
      "\n",
      "Entity relationships extracted from sentences individually:\n",
      "Warsaw -- contains administrative territorial entity -> Śródmieście\n",
      "Śródmieście -- located in the administrative territorial entity -> Warsaw\n",
      "2006 -- point in time -> 2006\n",
      "Warsaw -- capital of -> regionally\n",
      "regionally -- capital -> Warsaw\n",
      "MasterCard Emerging Market Index -- located in the administrative territorial entity -> Warsaw\n",
      "Warsaw -- part of -> 7th greatest emerging market\n",
      "2002 -- point in time -> 2002\n",
      "Warsaw -- country -> Poland\n",
      "Poland -- capital -> Warsaw\n",
      "Poland -- member of -> European Union\n",
      "European Union -- contains administrative territorial entity -> Poland\n",
      "Warsaw -- instance of -> PLN\n",
      "Polish average -- instance of -> nominal GDP\n",
      "Warsaw -- part of -> East-Central Europe\n",
      "East-Central Europe -- capital -> Warsaw\n",
      "first quarter of 2008 -- point in time -> 2008\n",
      "\n",
      "Entity relationships extracted from the entire paragraph:\n",
      "Warsaw -- contains administrative territorial entity -> Śródmieście\n",
      "Śródmieście -- located in the administrative territorial entity -> Warsaw\n",
      "Śródmieście -- located in the administrative territorial entity -> Warsaw\n",
      "Śródmieście -- located in the administrative territorial entity -> Warsaw\n",
      "Warsaw -- contains administrative territorial entity -> Śródmieście\n",
      "Warsaw -- contains administrative territorial entity -> Śródmieście\n"
     ]
    }
   ],
   "source": [
    "# test paragraph or sentence entity relationships\n",
    "with get_db_con().cursor() as cur:\n",
    "    content_ids, all_content = get_all_content(cur)\n",
    "random_paragraph = random.choice(all_content)\n",
    "sentences = sentence_extractor.get_sentences(random_paragraph)\n",
    "\n",
    "# Entity extraction on each sentence individually\n",
    "individual_triplets = []\n",
    "for sentence in sentences:\n",
    "    triplets = er_extractor.extract_ERs([sentence])\n",
    "    individual_triplets.extend(triplets)\n",
    "\n",
    "# Entity extraction on the entire paragraph as one\n",
    "paragraph_triplets = er_extractor.extract_ERs([random_paragraph])\n",
    "\n",
    "# Print results\n",
    "print(\"Random Paragraph:\\n\")\n",
    "jupyter_print_paragraph(random_paragraph)\n",
    "print(\"\\nEntity relationships extracted from sentences individually:\")\n",
    "for triplet in individual_triplets:\n",
    "    print(f\"{triplet.head} -- {triplet.type} -> {triplet.tail}\")\n",
    "\n",
    "print(\"\\nEntity relationships extracted from the entire paragraph:\")\n",
    "for triplet in paragraph_triplets:\n",
    "    print(f\"{triplet.head} -- {triplet.type} -> {triplet.tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1432b8",
   "metadata": {},
   "source": [
    "# Basic Semantic Embedding Example\n",
    "Demonstrates how sentance encoding works and what kind of values this system outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58811c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star wars begins on episode 4 with a jedi who must travel to a different planet\n",
      "22.31% - A long time ago in a galaxy far far away\n",
      "35.85% - There was a space ship floating with anakin on it\n",
      "9.22% - A farmer milks his cows\n",
      "5.74% - they actually were on his land\n",
      "-0.87% - When a phone falls down into a drain, something interesting happens\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "test_passages = [\n",
    "    \"A long time ago in a galaxy far far away\",\n",
    "    \"There was a space ship floating with anakin on it\",\n",
    "    \"A farmer milks his cows\",\n",
    "    \"they actually were on his land\",\n",
    "    \"When a phone falls down into a drain, something interesting happens\",\n",
    "]\n",
    "test_query = [\n",
    "    \"Star wars begins on episode 4 with a jedi who must travel to a different planet\"\n",
    "]\n",
    "\n",
    "passage_embeddings = model.encode(test_passages)\n",
    "query_embeddings = model.encode(test_query)\n",
    "\n",
    "# Compute the (cosine) similarity scores\n",
    "scores = model.similarity(query_embeddings, passage_embeddings) * 100\n",
    "\n",
    "# Print the test passages with their corresponding similarities\n",
    "print(test_query[0])\n",
    "for passage, score in zip(test_passages, scores.tolist()[0]):\n",
    "    print(f\"{round(score, 2)}% - {passage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab7edd",
   "metadata": {},
   "source": [
    "# Test Neo4j Content ID merge\n",
    "To track which content blocks are referenced in the neo4j database, we need to make sure merges are being handled by the insert relationship function correctly\n",
    "In order to do so, two nodes are added from documents 1 and 2. If everything goes according to plan, both nodes will have doc1 and doc2 listed in their content id field of their properties. It is worth noting that this is being ran against the actual neo4j database so they need to be removed after the test has been preformed. Please run the insert, inspect to make sure it was handled correctly, and then remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3343b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neo4j.insert_relationship(\n",
    "    subject=\"John\",\n",
    "    subject_type=\"Person\",\n",
    "    predicate=\"KNOWS\",\n",
    "    object=\"Jane\",\n",
    "    object_type=\"Person\",\n",
    "    content_id=\"doc1\"\n",
    ")\n",
    "\n",
    "result = neo4j.insert_relationship(\n",
    "    subject=\"John\",\n",
    "    subject_type=\"Person\",\n",
    "    predicate=\"KNOWS\",\n",
    "    object=\"Jane\",\n",
    "    object_type=\"Person\",\n",
    "    content_id=\"doc2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fedc448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up\n",
    "neo4j.remove_node_by_name(\"John\")\n",
    "neo4j.remove_node_by_name(\"Jane\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HybridRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
