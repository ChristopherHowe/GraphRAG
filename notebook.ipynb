{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_funcs import get_all_content, get_db_con\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "conn=get_db_con()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Chunk Size\n",
    "Determine what kinda size the current article chunks are, decided that for now, chunking is unnecesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 802.61\n",
      "Maximum length: 4063\n",
      "Minimum length: 169\n",
      "Standard deviation: 359.58\n",
      "\n",
      "Sample of lengths: [717, 658, 610, 582, 1521, 715, 617, 585, 531, 522, 623, 1042, 750, 1087, 833, 688, 483, 765, 876, 561]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "with conn.cursor() as cur:\n",
    "    content_ids, all_content = get_all_content(cur)\n",
    "content_lengths = [len(content) for content in all_content]\n",
    "\n",
    "print(f\"Average length: {np.mean(content_lengths):.2f}\")\n",
    "print(f\"Maximum length: {max(content_lengths)}\")\n",
    "print(f\"Minimum length: {min(content_lengths)}\")\n",
    "print(f\"Standard deviation: {np.std(content_lengths):.2f}\")\n",
    "print(f\"\\nSample of lengths: {random.sample(content_lengths, min(20, len(content_lengths)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentances and Entity Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/GraphRAG/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to extract sentences from content\n",
      "Extracted all sentences from content in 0.19 seconds\n",
      "Batch 1 of 64 complete\n",
      "Batch 2 of 64 complete\n",
      "Batch 3 of 64 complete\n",
      "Batch 4 of 64 complete\n",
      "Batch 5 of 64 complete\n",
      "Batch 6 of 64 complete\n",
      "Batch 7 of 64 complete\n",
      "Batch 8 of 64 complete\n",
      "Batch 9 of 64 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative time after 10 batches:\n",
      "  Decoding: 250.87 seconds\n",
      "  Extracting: 0.01 seconds\n",
      "  Inserting: 8.08 seconds\n",
      "Batch 10 of 64 complete\n",
      "Batch 11 of 64 complete\n",
      "Batch 12 of 64 complete\n",
      "Batch 13 of 64 complete\n",
      "Batch 14 of 64 complete\n",
      "Batch 15 of 64 complete\n",
      "Batch 16 of 64 complete\n",
      "Batch 17 of 64 complete\n",
      "Batch 18 of 64 complete\n",
      "Batch 19 of 64 complete\n",
      "\n",
      "Cumulative time after 20 batches:\n",
      "  Decoding: 475.26 seconds\n",
      "  Extracting: 0.01 seconds\n",
      "  Inserting: 15.65 seconds\n",
      "Batch 20 of 64 complete\n",
      "Batch 21 of 64 complete\n",
      "Batch 22 of 64 complete\n",
      "Batch 23 of 64 complete\n",
      "Batch 24 of 64 complete\n",
      "Batch 25 of 64 complete\n",
      "Batch 26 of 64 complete\n",
      "Batch 27 of 64 complete\n",
      "Batch 28 of 64 complete\n",
      "Batch 29 of 64 complete\n",
      "\n",
      "Cumulative time after 30 batches:\n",
      "  Decoding: 696.37 seconds\n",
      "  Extracting: 0.02 seconds\n",
      "  Inserting: 25.17 seconds\n",
      "Batch 30 of 64 complete\n",
      "Batch 31 of 64 complete\n",
      "Batch 32 of 64 complete\n",
      "Batch 33 of 64 complete\n",
      "Batch 34 of 64 complete\n",
      "Batch 35 of 64 complete\n",
      "Batch 36 of 64 complete\n",
      "Batch 37 of 64 complete\n",
      "Batch 38 of 64 complete\n",
      "Batch 39 of 64 complete\n",
      "\n",
      "Cumulative time after 40 batches:\n",
      "  Decoding: 923.71 seconds\n",
      "  Extracting: 0.02 seconds\n",
      "  Inserting: 37.06 seconds\n",
      "Batch 40 of 64 complete\n",
      "Batch 41 of 64 complete\n",
      "Batch 42 of 64 complete\n",
      "Batch 43 of 64 complete\n",
      "Batch 44 of 64 complete\n",
      "Batch 45 of 64 complete\n",
      "Batch 46 of 64 complete\n",
      "Batch 47 of 64 complete\n",
      "Batch 48 of 64 complete\n",
      "Batch 49 of 64 complete\n",
      "\n",
      "Cumulative time after 50 batches:\n",
      "  Decoding: 1177.69 seconds\n",
      "  Extracting: 0.03 seconds\n",
      "  Inserting: 52.70 seconds\n",
      "Batch 50 of 64 complete\n",
      "Batch 51 of 64 complete\n",
      "Batch 52 of 64 complete\n",
      "Batch 53 of 64 complete\n",
      "Batch 54 of 64 complete\n",
      "Batch 55 of 64 complete\n",
      "Batch 56 of 64 complete\n",
      "Batch 57 of 64 complete\n",
      "Batch 58 of 64 complete\n",
      "Batch 59 of 64 complete\n",
      "\n",
      "Cumulative time after 60 batches:\n",
      "  Decoding: 1437.58 seconds\n",
      "  Extracting: 0.03 seconds\n",
      "  Inserting: 69.30 seconds\n",
      "Batch 60 of 64 complete\n",
      "Batch 61 of 64 complete\n",
      "Batch 62 of 64 complete\n",
      "Batch 63 of 64 complete\n",
      "Batch 64 of 64 complete\n",
      "\n",
      "Total time spent decoding: 1515.96 seconds\n",
      "Total time spent extracting: 0.04 seconds\n",
      "Total time spent inserting: 75.58 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from utils import extract_triplets, Sentence_Extractor\n",
    "from neo4j_funcs import Neo4j_Driver\n",
    "from typing import List\n",
    "from models import triplet\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sentence_extractor=Sentence_Extractor()\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "neo4j = Neo4j_Driver()\n",
    "\n",
    "sentences=[]\n",
    "print(\"Starting to extract sentences from content\")\n",
    "start_time = time.time()\n",
    "for content in all_content:\n",
    "    sentences.append(sentence_extractor.get_sentences(content))\n",
    "end_time = time.time()\n",
    "print(f\"Extracted all sentences from content in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = [s for content_sentences in sentences for s in content_sentences]  # Flatten list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "dataset = SentenceDataset(sentences)\n",
    "batch_size=100\n",
    "sentences_dataloader=DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "total_batches = (len(dataset) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "timeDecoding = 0\n",
    "timeExtracting = 0\n",
    "timeInserting = 0\n",
    "\n",
    "for batch_idx, sentences_batch in enumerate(sentences_dataloader):\n",
    "    start_decoding = time.time()\n",
    "    tokens = triplet_extractor(sentences_batch, return_tensors=True, return_text=False)\n",
    "    token_ids = [token[\"generated_token_ids\"] for token in tokens]\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode(token_ids)\n",
    "    end_decoding = time.time()\n",
    "    timeDecoding += (end_decoding - start_decoding)\n",
    "\n",
    "    start_extracting = time.time()\n",
    "    triplets = [triplet for text in extracted_text for triplet in extract_triplets(text)]\n",
    "    end_extracting = time.time()\n",
    "    timeExtracting += (end_extracting - start_extracting)\n",
    "\n",
    "    start_inserting = time.time()\n",
    "    for e in triplets:\n",
    "        neo4j.insert_relationship(e.head, \"entity\", e.type, e.tail, \"entity\")\n",
    "    end_inserting = time.time()\n",
    "    timeInserting += (end_inserting - start_inserting)\n",
    "\n",
    "    if (batch_idx + 1) % 10 == 0:\n",
    "        print(f\"\\nCumulative time after {batch_idx + 1} batches:\")\n",
    "        print(f\"  Decoding: {timeDecoding:.2f} seconds\")\n",
    "        print(f\"  Extracting: {timeExtracting:.2f} seconds\")\n",
    "        print(f\"  Inserting: {timeInserting:.2f} seconds\")\n",
    "\n",
    "    print(f\"Batch {batch_idx + 1} of {total_batches} complete\")\n",
    "\n",
    "print(f\"\\nTotal time spent decoding: {timeDecoding:.2f} seconds\")\n",
    "print(f\"Total time spent extracting: {timeExtracting:.2f} seconds\")\n",
    "print(f\"Total time spent inserting: {timeInserting:.2f} seconds\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
